

## Image Classification
<a href="https://cs231n.github.io/classification/"> 课程notes </a>
### 课程大纲
* Image Classification
  * Nearest Neighbor Classifier
  * k - Nearest Neighbor Classifier
  * Validation sets for Hyperparameter tuning
  * Summary
  * Summary: Applying kNN in practice
  * Further Reading
### Nearest Neighbor Classifier
假设现在我们有CIFAR-10的50000张图片（每种分类5000张）作为训练集，我们希望将余下的10000作为测试集并给他们打上标签。Nearest Neighbor算法将会拿着测试图片和训练集中每一张图片去比较，然后将它认为最相似的那个训练集图片的标签赋给这张测试图片。以图片中的一个颜色通道为例来进行说明。
两张图片使用L1距离来进行比较。逐个像素求差值，然后将所有差值加起来得到一个数值。如果两张图片一模一样，那么L1距离为0，但是如果两张图片很是不同，那L1值将会非常大。

#### 距离选择
计算向量间的距离有很多种方法，另一个常用的方法是L2距离，从几何学的角度，可以理解为它在计算两个向量间的欧式距离。
L1和L2比较。比较这两个度量方式是挺有意思的。在面对两个向量之间的差异时，L2比L1更加不能容忍这些差异。也就是说，相对于1个巨大的差异，L2距离更倾向于接受多个中等程度的差异。L1和L2都是在p-norm常用的特殊形式。

### k - Nearest Neighbor Classifier
与其只找最相近的那1个图片的标签，我们找最相似的k个图片的标签，然后让他们针对测试图片进行投票，最后把票数最高的标签作为对测试图片的预测。所以当k=1的时候，k-Nearest Neighbor分类器就是Nearest Neighbor分类器。

### Validation sets for Hyperparameter tuning
k如何选择，cross-validation，K折验证

#### 优劣
##### 优点
* Nearest Neighbor分类器易于理解，实现简单。
* 算法的训练不需要花时间，因为其训练过程只是将训练集数据存储起来

##### 缺点
在CV领域不用：
* 预测耗时
* L1或L2距离不能很好的表达相似性
* 维度灾难

### Summary
* 介绍了图像分类问题。在该问题中，给出一个由被标注了分类标签的图像组成的集合，要求算法能预测没有标签的图像的分类标签，并根据算法预测准确率进行评价。
* 介绍了一个简单的图像分类器：最近邻分类器(Nearest Neighbor classifier)。分类器中存在不同的超参数(比如k值或距离类型的选取)，要想选取好的超参数不是一件轻而易举的事。
* 选取超参数的正确方法是：将原始训练集分为训练集和验证集，我们在验证集上尝试不同的超参数，最后保留表现最好那个。
* 如果训练数据量不够，使用交叉验证方法，它能帮助我们在选取最优超参数的时候减少噪音。
* 一旦找到最优的超参数，就让算法以该参数在测试集跑且只跑一次，并根据测试结果评价算法。
* 最近邻分类器能够在CIFAR-10上得到将近40%的准确率。该算法简单易实现，但需要存储所有训练数据，并且在测试的时候过于耗费计算能力。
* 最后，我们知道了仅仅使用L1和L2范数来进行像素比较是不够的，图像更多的是按照背景和颜色被分类，而不是语义主体分身。

### Summary: Applying kNN in practice
如果你希望将k-NN分类器用到实处（最好别用到图像上，若是仅仅作为练手还可以接受），那么可以按照以下流程：

* 预处理你的数据：对你数据中的特征进行归一化（normalize），让其具有零平均值（zero mean）和单位方差（unit variance）。在后面的小节我们会讨论这些细节。本小节不讨论，是因为图像中的像素都是同质的，不会表现出较大的差异分布，也就不需要标准化处理了。
* 如果数据是高维数据，考虑使用降维方法，比如PCA(wiki ref, CS229ref, blog ref)或随机投影。
* 将数据随机分入训练集和验证集。按照一般规律，70%-90% 数据作为训练集。这个比例根据算法中有多少超参数，以及这些超参数对于算法的预期影响来决定。如果需要预测的超参数很多，那么就应该使用更大的验证集来有效地估计它们。如果担心验证集数量不够，那么就尝试交叉验证方法。如果计算资源足够，使用交叉验证总是更加安全的（份数越多，效果越好，也更耗费计算资源）。
在验证集上调优，尝试足够多的k值，尝试L1和L2两种范数计算方式。
* 如果分类器跑得太慢，尝试使用Approximate Nearest Neighbor库（比如FLANN）来加速这个过程，其代价是降低一些准确率。
* 对最优的超参数做记录。记录最优参数后，是否应该让使用最优参数的算法在完整的训练集上运行并再次训练呢？因为如果把验证集重新放回到训练集中（自然训练集的数据量就又变大了），有可能最优参数又会有所变化。在实践中，不要这样做。千万不要在最终的分类器中使用验证集数据，这样做会破坏对于最优参数的估计。直接使用测试集来测试用最优参数设置好的最优模型，得到测试集数据的分类准确率，并以此作为你的kNN分类器在该数据上的性能表现。

### Further Reading
A Few Useful Things to Know about Machine Learning
Recognizing and Learning Object Categories


# cs231N 相关资料
课程官网：<a href="http://cs231n.stanford.edu/" > CS231n: Convolutional Neural Networks for Visual Recognition </a> 
Github：[https://github.com/cs231n/cs231n.github.io | http://cs231n.github.io/  ](https://cs231n.github.io/)
教学安排及大纲：Schedule and Syllabus 课程视频：Youtube上查看Andrej Karpathy创建的播放列表，或者网易云课堂  
课程pdf及视频下载：<a href="https://pan.baidu.com/disk/main?from=homeSave#/index?category=all&path=%2F%E8%87%AA%E6%88%91%E6%8F%90%E5%8D%87%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2FCS231n%20-%20Convolutional%20Neural%20Networks%20for%20Visual%20Recognition%28Winter%202016%29" > 百度网盘 </a>
CS231课程笔记翻译 https://zhuanlan.zhihu.com/intelligentunit
