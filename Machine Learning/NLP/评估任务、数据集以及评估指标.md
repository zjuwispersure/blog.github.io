# NLP主要任务
* 序列标注:分词，词性标注，命名实体识别
* 分类:文本分类，情感计算
* 句子关系:entailment（分类为蕴含或矛盾），相似度计算
* 文本生成:机器翻译，问答系统，文本摘要
  



# 文本生成评估指标
##  BLEU
  > Bleu是IBM在2002提出的，用于机器翻译任务的评价，发表在ACL，原文题目是“BLEU: a Method for Automatic Evaluation of Machine Translation”。

它的总体思想就是准确率，假如给定标准译文reference，神经网络生成的句子是candidate，句子长度为n，candidate中有m个单词出现在reference，m/n就是bleu的1-gram的计算公式。  
  BLEU还有许多变种。根据n-gram可以划分成多种评价指标，常见的指标有BLEU-1、BLEU-2、BLEU-3、BLEU-4四种，其中n-gram指的是连续的单词个数为n。BLEU-1衡量的是单词级别的准确性，更高阶的bleu可以衡量句子的流畅性。  

### 计算公式

![image](https://github.com/zjuwispersure/zjuwispersure.github.io/assets/3489254/54cdf86c-c626-4953-8b41-f0125af3a15b)
![image](https://github.com/zjuwispersure/zjuwispersure.github.io/assets/3489254/6dedaa6d-3375-4ab8-a25f-0fb7772d8274)


* BP是简短惩罚因子，惩罚一句话的长度过短，防止训练结果倾向短句的现象。以c来表示待评价译文的长度，r来表示参考译文的文字长度 
* Pn是基于modified n-gram recision的精确度,主要思路是Reference语句里面如果一个单词片段已经被匹配，那么这个片段就不能再次被匹配，并且一个单词片段只能取一个Reference语句中出现次数的最大值，比如7个the分别在Reference 1 和 2中出现2和1次，所以取2而不是两者相加的3。

### 优缺点
* 优点：计算速度快、计算成本低、容易理解、与具体语言无关、和人类给的评估高度相关。
* 缺点：不考虑语言表达（语法）上的准确性；测评精度会受常用词的干扰；短译句的测评精度有时会较高；没有考虑同义词或相似表达的情况，可能会导致合理翻译被否定。

## Rouge
> ROUGE（Recall-Oriented Understudy for Gisting Evaluation）评估指标最早出自chin-yew lin在2003年的论文《ROUGE: Recall-oriented understudy for gisting evaluation》
> 
  ROUGE指标是在机器翻译、自动摘要、问答生成等领域常见的评估指标。
  ROUGE通过将模型生成的摘要或者回答与参考答案（一般是人工生成的）进行比较计算，得到对应的得分。
  ROUGE指标与BLEU指标非常类似，均可用来衡量生成结果和标准结果的匹配程度，不同的是ROUGE基于召回率，BLEU更看重准确率。
  在论文中主要提到了4种方法，分别是Rouge-N、Rouge-L、Rouge-W、Rouge-S。


### ROUGE-N
ROUGE-N 指标计算生成的摘要与相应的参考摘要的 n-gram 召回率，具体的公式为：
![image](https://github.com/zjuwispersure/zjuwispersure.github.io/assets/3489254/59c6d2f1-38b0-4ca3-bc6f-0b0ee3f89529)

其中分母部分计算参考摘要中 n-gram 的个数，分子部分计算参考摘要和自动摘要共有的 n-gram 的个数。

实例：
自动摘要：the cat was found under the bed
参考摘要：the cat was under the bed

![image](https://github.com/zjuwispersure/zjuwispersure.github.io/assets/3489254/8f390526-b886-48b9-898b-91d002ec59b1)

> ROUGE-1 = 6/6 = 1   
ROUGE-2 = 4/5 = 0.8

### ROUGE-L
ROUGE-L 指标基于两个文本单元的最长公共序列，计算 F-measure，具体公式如下：

![image](https://github.com/zjuwispersure/zjuwispersure.github.io/assets/3489254/1fbf104e-fa49-4ff1-bc85-7ec6b5fcf56d)

其中 X 为参考摘要，长度为 m；Y 为生成摘要，长度为 n；β 为精确率和召回率的比值。

>实例：
自动摘要：police kill the gunman  
参考摘要：police killed the gunman   
R = 3/4   
P = 3/4   
ROUGH-L = F = 3/4 = 0.75   

### ROUGE-W
ROUGE-W 指标在 ROUGE-L 的基础上进行加权计算。

X: [A B C D E F G]
Y1: [A B C D H I K]
Y2: [A H B K C I D]
Y1 和 Y2 的 ROUGH-L 值都为 4/7，但明显 Y1 与参考摘要更加接近，所以作者提出了一个基于最长公共子序列的加权算法。

![image](https://github.com/zjuwispersure/zjuwispersure.github.io/assets/3489254/05e2f058-6771-4267-811b-a01812707a05)


### ROUGH-S
ROUGH-S 使用了 skip-grams，在参考摘要和生成摘要进行进行匹配时，不要求 gram 之间是连续的，可跳过几个单词，如 skip-bigram，在产生 grams 时，允许最多跳过两个词。

![image](https://github.com/zjuwispersure/zjuwispersure.github.io/assets/3489254/86e16759-2508-44ba-9c29-237dc67bebd9)


>实例：
>例子：cat in the hat
>skip-bigrams：cat in, cat the, cat hat, in the, in hat, the hat


### 优缺点
* 缺点 
是这种方法只能在单词、短语的角度去衡量两个句子的形似度。并不能支持同义词、近义词等语意级别去衡量。比如：  
ref = "I'm very happy!"  
hyp = "I'm very sad!"  
hyp1 = "I'm very cheerful!"  
hyp1和hyp2的rouge得分是一样的。但显然hyp1才是与ref更相近的。
* 优点
是这种方式计算高效，在忽略近义词等情况下，做到比较合理的判断。

## DST评估指标
评价指标
多领域问答的性能评价指标简单介绍如下：

1.4.1 传统指标
传统的任务型问答系统的衡量指标包括任务完成度指标（Task-completion metrics）、词重叠评价指标等。人工评估结果也是很重要的方法。

任务完成度指标，包括Inform（实体匹配率，entity matching rate）和Success（客观任务成功率，objective task success rate）。Inform衡量了系统提供恰当实体的能力。Success衡量系统回答所有被请求属性(requested attributes)的能力。通过确定每个对话结束时实际选择的实体是否与用户指定的任务相匹配来计算实体匹配率。如果（1）所提供的实体匹配，且（2）系统回答来自用户的所有相关信息请求，则对话被标记为成功。

词重叠评价指标，目前基本就是BLEU分数。BLEU本身来自机器翻译领域，用于衡量一条翻译的生成响应与真实响应的相似性，后来问答系统借用了这个指标。

1.4.2 多领域DST指标
就多领域DST而言，如何衡量多领域对话状态跟踪的性能，目前也有很多的流派和争议。目前一种比较常用的评价指标叫作联合准确率，Joint accuracy。

联合准确率用于评估联合状态跟踪的性能。对每一回合，当且仅当全部的< domain, slot, value > 三元组被正确预测时，认为对话状态预测正确，联合准确率等于1，否则等于0。

除了联合准确率以外，槽位准确率（Slot accuracy）也常常被用作衡量多领域DST的指标之一。槽位准确率关注的是slot-level的预测能力，对一个(domain, slot, value)，当且仅当 domain-slot pair的value被正确预测时， 认为这个单个的slot预测正确。

通过比较表述可知，联合准确率的提升比槽位准确率更困难，也更加重要。纵览截止今年10月的现有多领域问答模型，联合准确率基本还处在30%到50%的区间上下，而槽位准确率很早就达到了97%以上。

联合准确率在不同论文中会叫各种名字，如联合状态准确率（joint state accuracy）、联合目标准确率（joint goal accuracy）等，含义是相同的。

在COMER的论文中，另外存在一套更加细致的，对domain,slot,value层层递进地衡量准确率的指标。分别定义联合领域准确率（joint domain accuracy，简记为JD ACC.）、联合域-槽准确率（joint domain-slot accuracy，简记为JDS ACC.）和联合目标准确率（joint goal accuracy，简记为JG ACC.）。JD ACC.表示所有领域预测正确的概率，JDS ACC.表示所有领域及槽位正确的概率。
那么，给定正确领域，slots被正确预测的概率就是JDSACC.JDACC.\dfrac {JDS\ ACC.}{JD\ ACC.}JD ACC.JDS ACC.​

同理，给定正确(领域和)槽位，values被正确预测的概率就是JGACC.JDSACC.\dfrac {JG\ ACC.}{JDS\ ACC.}JDS ACC.JG ACC.​
上述指标可以用于衡量DST模型domain prediction、slot prediction和value prediction的能力。

为了衡量在相同本体复杂性下，DST模型的运行效率，COMER模型的研究团队还在他们的论文中提出了推断时间复杂度(inference time complexity,ITC)的概念，以衡量不同DST模型对话状态预测的效率。ITC指的是完成一次对话状态预测，需要inference多少次。显然ITC是越小越好


# 常见32项NLP评估任务

持续调整中，很多任务有多个数据集，切换到paperwitchcoder上的内容。

| 任务 | 描述 | corpus/dataset | 评价指标 | SOTA结果 | Papers |
|-----|----------|---------------|---------|----------|--------|
| Chunking | 组块分析 | Penn Treebank | F1 | 95.77 | A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks |
| Common sense reasoning | 常识推理 | Event2Mind | cross-entropy | 4.22 | Event2Mind: Commonsense Inference on Events, Intents, and Reactions |
| Parsing | 句法分析 | Penn Treebank | F1 | 95.13 | Constituency Parsing with a Self-Attentive Encoder |
| Coreference resolution | 指代消解 | CoNLL 2012 | average F1 | 73 | Higher-order Coreference Resolution with Coarse-to-fine Inference |
| Dependency parsing | 依存句法分析 | Penn Treebank | POSUASLAS | 97.395.4493.76 | Deep Biaffine Attention for Neural Dependency Parsing |
| Task-Oriented Dialogue/Intent Detection | 任务型对话/意图识别 | ATIS/Snips | accuracy | 94.1 97.0 | Slot-Gated Modeling for Joint Slot Filling and Intent Prediction |
| Task-Oriented Dialogue/Slot Filling | 任务型对话/槽填充 | ATIS/Snips | F1 | 95.288.8 | Slot-Gated Modeling for Joint Slot Filling and Intent Prediction |
| Task-Oriented Dialogue/Dialogue State Tracking | 任务型对话/状态追踪 | DSTC2 | AreaFoodPriceJoint | 90849272 | Dialogue Learning with Human Teaching and Feedback in End-to-End Trainable Task-Oriented Dialogue Systems |
| Domain adaptation | 领域适配 | Multi-Domain Sentiment Dataset | average accuracy | 79.15 | Strong Baselines for Neural Semi-supervised Learning under Domain Shift |
| Entity Linking | 实体链接 | AIDA CoNLL-YAGO | Micro-F1-strongMacro-F1-strong | 86.6 89.4 | End-to-End Neural Entity Linking |
| Information Extraction | 信息抽取 | ReVerb45K | PrecisionRecallF1 | 62.784.481.9 | CESI: Canonicalizing Open Knowledge Bases using Embeddings and Side Information |
| Grammatical Error Correction | 语法错误纠正 | JFLEG | GLEU | 61.5 | Near Human-Level Performance in Grammatical Error Correction with Hybrid Machine Translation |
| Language modeling | 语言模型 | Penn Treebank | Validation perplexity Test perplexity | 48.3347.69 | Breaking the Softmax Bottleneck: A High-Rank RNN Language Model |
| Lexical Normalization | 词汇规范化 | LexNorm2015 | F1PrecisionRecall | 86.39 93.53 80.26 | MoNoise: Modeling Noise Using a Modular Normalization System |
| Machine translation | 机器翻译 | WMT 2014 EN-DE | BLEU | 35.0 | Understanding Back-Translation at Scale |
| Multimodal Emotion Recognition | 多模态情感识别 | IEMOCAP | Accuracy | 76.5 | Multimodal Sentiment Analysis using Hierarchical Fusion with Context Modeling |
| Multimodal Metaphor Recognition | 多模态隐喻识别 | verb-noun pairs adjective-noun pairs | F1 | 0.750.79 | Black Holes and White Rabbits: Metaphor Identification with Visual Features |
| Multimodal Sentiment Analysis | 多模态情感分析 | MOSI | Accuracy | 80.3 | Context-Dependent Sentiment Analysis in User-Generated Videos |
| Named entity recognition | 命名实体识别 | CoNLL 2003 | F1 | 93.09 | Contextual String Embeddings for Sequence Labeling |
| Natural language inference | 自然语言推理 | SciTail | Accuracy | 88.3 | Improving Language Understanding by Generative Pre-Training |
| Part-of-speech tagging | 词性标注 | Penn Treebank | Accuracy | 97.96 | Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings |
| Question answering | 问答 | CliCR | F1 | 33.9 | CliCR: A Dataset of Clinical Case Reports for Machine Reading Comprehension |
| Word segmentation | 分词 | VLSP 2013 | F1 | 97.90 | A Fast and Accurate Vietnamese Word Segmenter |
| Word Sense Disambiguation | 词义消歧 | SemEval 2015 | F1 | 67.1 | Word Sense Disambiguation: A Unified Evaluation Framework and Empirical Comparison |
| Text classification | 文本分类 | AG News | Error rate | 5.01 | Universal Language Model Fine-tuning for Text Classification |
| Summarization | 摘要 | Gigaword | ROUGE-1ROUGE-2ROUGE-L | 37.0419.0334.46 | Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization |
| Sentiment analysis | 情感分析 | IMDb | Accuracy | 95.4 | Universal Language Model Fine-tuning for Text Classification |
| Semantic role labeling | 语义角色标注 | OntoNotes | F1 | 85.5 | Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling |
| Semantic parsing | 语义解析 | LDC2014T12 | F1 NewswireF1 Full | 0.710.66 | AMR Parsing with an Incremental Joint Model |
| Semantic textual similarity | 语义文本相似度 | SentEval | MRPCSICK-RSICK-ESTS | 78.6/84.40.88887.878.9/78.6 | Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning |
| Relationship Extraction | 关系抽取 | New York Times Corpus | P@10%P@30% | 73.659.5 | RESIDE: Improving Distantly-Supervised Neural Relation Extraction using Side Information |
| Relation Prediction | 关系预测 | WN18RR | H@10H@1MRR | 59.0245.3749.83 | Predicting Semantic Relations using Global Graph Properties |



# 参考文档
跟踪NLP进展的博客<a href="https://github.com/sebastianruder/NLP-progress" > Tracking Progress in Natural Language Processing </a> (近期无更新)  
评估任务SOTA <a href="https://paperswithcode.com/sota"> paperswithcode </a>
